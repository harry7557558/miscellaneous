{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-30T00:05:35.864681Z","iopub.execute_input":"2022-07-30T00:05:35.865050Z","iopub.status.idle":"2022-07-30T00:05:35.873733Z","shell.execute_reply.started":"2022-07-30T00:05:35.865011Z","shell.execute_reply":"2022-07-30T00:05:35.872612Z"},"trusted":true},"execution_count":274,"outputs":[]},{"cell_type":"code","source":"import requests\n\ndef image_loader(url, filename):\n    req = requests.get(url)\n    print(req.status_code)\n    with open(filename, 'wb') as fp:\n        fp.write(req.content)\n    image = Image.open(filename).convert(\"RGB\")\n    image = np.array(image.resize((256, 256))) / 255.0\n    plt.imshow(image)\n    plt.show()\n    image = np.einsum('abc->cab', image)\n    image = torch.tensor(image).unsqueeze(0)\n    image = image.contiguous()  # https://discuss.pytorch.org/t/error-when-running-lbfgs-to-solve-a-non-linear-inverse-problem/99911/3\n    image = image.to(device, torch.float)\n    return image\n\n\npreset = [\n    # Desmos \"Ammonite\" in the style of \"The Great Waves\"\n    (\n        \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Tsunami_by_hokusai_19th_century.jpg/525px-Tsunami_by_hokusai_19th_century.jpg\",\n        \"https://saved-work.desmos.com/calc_thumbs/production/z7zooq9zsh.png\"\n    ),\n    # \"Functional Conch\" in the style of \"The Great Waves\"\n    (\n        \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Tsunami_by_hokusai_19th_century.jpg/525px-Tsunami_by_hokusai_19th_century.jpg\",\n        \"https://cdn.discordapp.com/attachments/987164635872526407/993715550226227200/profile-conch-256-256.png\"\n    ),\n    # \"Functional Conch\" in the style of \"A Sunday on La Grande Jatte\"\n    (\n        \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/A_Sunday_on_La_Grande_Jatte%2C_Georges_Seurat%2C_1884.jpg/1280px-A_Sunday_on_La_Grande_Jatte%2C_Georges_Seurat%2C_1884.jpg\",\n        \"https://cdn.discordapp.com/attachments/987164635872526407/993715550226227200/profile-conch-256-256.png\"\n    ),\n    # \"Functional Conch\" in the style of \"The Starry Night\" by Van Gogh\n    (\n        \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/405px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\",\n        \"https://cdn.discordapp.com/attachments/987164635872526407/993715550226227200/profile-conch-256-256.png\"\n    ),\n    # \"Triangled\" in the style of \"The Great Waves\"\n    (\n        \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Tsunami_by_hokusai_19th_century.jpg/525px-Tsunami_by_hokusai_19th_century.jpg\",\n        \"https://cdn.discordapp.com/attachments/931320571453653072/1002723816478347274/unknown.png\"\n    ),\n    # \"Functional Conch\" in the style of \"Shot Marilyns\" by Andy Warhol\n    (\n        \"https://upload.wikimedia.org/wikipedia/en/5/5c/Shot_Marilyns.jpg\",\n        \"https://cdn.discordapp.com/attachments/987164635872526407/993715550226227200/profile-conch-256-256.png\"\n    ),\n    # \"Functional Conch\" in the style of \"Guernica\" by Picasso\n    (\n        \"https://upload.wikimedia.org/wikipedia/en/7/74/PicassoGuernica.jpg\",\n        \"https://cdn.discordapp.com/attachments/987164635872526407/993715550226227200/profile-conch-256-256.png\"\n    ),\n    # \"Functional Conch\" in the style of \"Mona Lisa\" by Da Vinci\n    (\n        \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/1200px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\",\n        \"https://cdn.discordapp.com/attachments/987164635872526407/993715550226227200/profile-conch-256-256.png\"\n    )\n][1]\n\nstyle_img = image_loader(preset[0], \"style.jpg\")\ncontent_img = image_loader(preset[1], \"content.jpg\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:05:36.482490Z","iopub.execute_input":"2022-07-30T00:05:36.483351Z","iopub.status.idle":"2022-07-30T00:05:37.754735Z","shell.execute_reply.started":"2022-07-30T00:05:36.483306Z","shell.execute_reply":"2022-07-30T00:05:37.753819Z"},"trusted":true},"execution_count":275,"outputs":[]},{"cell_type":"code","source":"# content loss, MSE\n\nclass ContentLoss(nn.Module):\n\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        # we 'detach' the target content from the tree used\n        # to dynamically compute the gradient: this is a stated value,\n        # not a variable. Otherwise the forward method of the criterion\n        # will throw an error.\n        self.target = target.detach()\n\n    def forward(self, input):\n        self.loss = nn.functional.mse_loss(input, self.target)\n        return input\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:05:37.756522Z","iopub.execute_input":"2022-07-30T00:05:37.757442Z","iopub.status.idle":"2022-07-30T00:05:37.764315Z","shell.execute_reply.started":"2022-07-30T00:05:37.757399Z","shell.execute_reply":"2022-07-30T00:05:37.763259Z"},"trusted":true},"execution_count":276,"outputs":[]},{"cell_type":"code","source":"# style loss\n\n# I don't get what the heck is this...\ndef gram_matrix(input):\n    a, b, c, d = input.size()  # a=batch size(=1)\n    # b=number of feature maps\n    # (c,d)=dimensions of a f. map (N=c*d)\n\n    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n\n    G = torch.mm(features, features.t())  # compute the gram product\n\n    # we 'normalize' the values of the gram matrix\n    # by dividing by the number of element in each feature maps.\n    return G.div(a * b * c * d)\n\n\nclass StyleLoss(nn.Module):\n\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = nn.functional.mse_loss(G, self.target)\n        return input\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:05:37.765693Z","iopub.execute_input":"2022-07-30T00:05:37.766080Z","iopub.status.idle":"2022-07-30T00:05:37.777655Z","shell.execute_reply.started":"2022-07-30T00:05:37.766027Z","shell.execute_reply":"2022-07-30T00:05:37.776525Z"},"trusted":true},"execution_count":277,"outputs":[]},{"cell_type":"code","source":"# Pre-trained model\n\n# We will use the features module because we need the output of the individual convolution layers\ncnn = models.vgg19(pretrained=True).features.to(device).eval()\n\n# The network is trained on batch normalized images\n# create a module to normalize input image so we can easily put it in a\n# nn.Sequential\nclass Normalization(nn.Module):\n    def __init__(self):\n        super(Normalization, self).__init__()\n        # .view the mean and std to make them [C x 1 x 1] so that they can\n        # directly work with image Tensor of shape [B x C x H x W].\n        # B is batch size. C is number of channels. H is height and W is width.\n        self.mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(-1, 1, 1)\n        self.std = torch.tensor([0.229, 0.224, 0.225]).to(device).view(-1, 1, 1)\n\n    def forward(self, img):\n        # normalize img\n        return (img - self.mean) / self.std\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:05:37.975510Z","iopub.execute_input":"2022-07-30T00:05:37.976307Z","iopub.status.idle":"2022-07-30T00:05:39.591152Z","shell.execute_reply.started":"2022-07-30T00:05:37.976265Z","shell.execute_reply":"2022-07-30T00:05:39.590141Z"},"trusted":true},"execution_count":278,"outputs":[]},{"cell_type":"code","source":"# Create model and loss\n\n# desired depth layers to compute style/content losses :\ncontent_layers_default = ['conv_4', 'conv_5']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    # normalization module\n    normalization = Normalization().to(device)\n\n    # just in order to have an iterable access to or list of content/syle\n    # losses\n    content_losses = []\n    style_losses = []\n\n    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n    # to put in modules that are supposed to be activated sequentially\n    model = nn.Sequential(normalization)\n\n    i = 0  # increment every time we see a conv\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            # The in-place version doesn't play very nicely with the ContentLoss\n            # and StyleLoss we insert below. So we replace with out-of-place\n            # ones here.\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            # add content loss:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            # add style loss:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    # now we trim off the layers after the last content and style losses\n    for i in range(len(model)-1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n    model = model[:(i + 1)]\n\n    return model, style_losses, content_losses\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:05:39.594006Z","iopub.execute_input":"2022-07-30T00:05:39.594638Z","iopub.status.idle":"2022-07-30T00:05:39.607846Z","shell.execute_reply.started":"2022-07-30T00:05:39.594599Z","shell.execute_reply":"2022-07-30T00:05:39.606889Z"},"trusted":true},"execution_count":279,"outputs":[]},{"cell_type":"code","source":"def run_style_transfer(cnn, content_img, style_img, input_img,\n                       num_steps=500, style_weight=1e3, content_weight=1e-2):\n    \"\"\"Run the style transfer.\"\"\"\n    print('Building the style transfer model..')\n    model, style_losses, content_losses = get_style_model_and_losses(\n        cnn, style_img, content_img)\n    input_img.requires_grad_(True)\n    model.requires_grad_(False)\n    optimizer = optim.LBFGS([input_img])\n\n    sigmoid = nn.Sigmoid()\n\n    def closure():\n        optimizer.zero_grad()\n        model(sigmoid(input_img))\n\n        style_score = 0.0\n        content_score = 0.0\n        for sl in style_losses:\n            style_score += sl.loss\n        for cl in content_losses:\n            content_score += cl.loss\n\n        style_score *= style_weight\n        content_score *= content_weight\n        loss = style_score + content_score\n        loss.backward()\n\n        run[0] += 1\n        if run[0] % 50 == 0:\n            print(\"run {}:\".format(run), end=' ')\n            print('Style Loss: {:4f} Content Loss: {:4f}'.format(\n                style_score.item(), content_score.item()))\n        return style_score + content_score\n\n    print('Optimizing..')\n    run = [0]\n    while run[0] <= num_steps:\n        optimizer.step(closure)\n\n    return sigmoid(input_img)\n\n\n#input_img = content_img.clone()\ninput_img = torch.rand(content_img.data.size(), device=device)\n\noutput = run_style_transfer(cnn, content_img, style_img, input_img)\noutput = output.cpu().detach().numpy()\noutput = np.einsum(\"abcd->cdb\", output)\nplt.figure()\nplt.imshow(output)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:05:39.610485Z","iopub.execute_input":"2022-07-30T00:05:39.610923Z","iopub.status.idle":"2022-07-30T00:05:48.865332Z","shell.execute_reply.started":"2022-07-30T00:05:39.610866Z","shell.execute_reply":"2022-07-30T00:05:48.864450Z"},"trusted":true},"execution_count":280,"outputs":[]}]}